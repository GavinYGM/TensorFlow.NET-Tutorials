# [C# TensorFlow 2 入门教程](<https://github.com/SciSharp/TensorFlow.NET-Tutorials>)

# 二、TensorFlow.NET API

## 8. 深度神经网络(DNN)入门

### 8.1 深度神经网络(DNN)介绍

2006年， 深度学习鼻祖Hinton在《SCIENCE 》上发表了一篇论文” [Reducing the Dimensionality of Data with Neural Networks](http://www.cs.toronto.edu/~hinton/science.pdf) “，这篇论文揭开了深度学习的序幕。 这篇论文提出了两个主要观点：（1）、多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原数据有更本质的代表性，这将大大便于分类和可视化问题；（2）、对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决，将上层训练好的结果作为下层训练过程中的初始化参数。 

深度神经网络(Deep Neural Networks，简称DNN)是深度学习的基础，想要学好深度学习，首先我们要理解DNN模型。 



**DNN模型结构：**

深度神经网络（Deep Neural Networks，DNN）可以理解为有很多隐藏层的神经网络，又被称为深度前馈网络（DFN），多层感知机（Multi-Layer Perceptron，MLP），其具有多层的网络结构，如下图所示： 

<img src="%E4%BA%8C%E3%80%81TensorFlow.NET%20API-8.%20%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(DNN)%E5%85%A5%E9%97%A8.assets/1598604214665.png" alt="1598604214665" style="zoom:50%;" />

DNN模型按照层的位置不同，可以分为3种神经网络层：输入层、隐藏层和输出层。一般来说，第一层为输入层，最后一层为输出层，中间为单个或多个隐藏层（某些模型的结构可能不同）。

层与层之间是**全连接的**，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部结构来看，它还是和普通的感知机一样，即一个线性函数搭配一个激活函数。 



**DNN前向传播：**

 利用若干个权重系数矩阵W，偏置向量 b 来和输入向量 X 进行一系列线性运算和激活运算，从输入层开始，一层层地向后计算，一直到运算到输出层，得到输出结果的值。 



**DNN反向传播：**

深度学习的过程即找到网络各层中最优的线性系数矩阵 W 和偏置向量 b，让所有的输入样本通过网络计算后，预测的输出尽可能等于或者接近实际的输出样本，这样的过程一般称为反向传播。

我们需要定义一个损失函数，来衡量预测输出值和实际输出值之间的差异。接着对这个损失函数进行优化，通过不断迭代对线性系数矩阵 W 和 偏置向量 b 进行更新，让损失函数最小化，不断逼近最小极值 或 满足我们预期的需求。在DNN中， 损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的。



**DNN在TensorFlow2.0中的一般流程：**

1. Step - 1：数据加载、归一化和预处理；
2. Step - 2：搭建深度神经网络模型；
3. Step - 3：定义损失函数和准确率函数；
4. Step - 4：模型训练；
5. Step - 5：模型预测推理，性能评估。



**DNN中的过拟合：**

随着网络的层数加深，模型的训练过程中会出现 梯度爆炸、梯度消失、欠拟合和过拟合，我们来说说比较常见的过拟合。过拟合一般是指模型的特征维度过多、参数过多，模型过于复杂，导致参数数量大大高于训练数据，训练出的网络层过于完美地适配训练集，但对新的未知的数据集的预测能力很差。即过度地拟合了训练数据，而没有考虑到模型的泛化能力。 

**一般的解决方法：**

-  获取更多数据：从数据源获得更多数据，或做数据增强；
- 数据预处理：清洗数据、减少特征维度、类别平衡；
- 正则化：限制权重过大、网络层数过多，避免模型过于复杂；
- 多种模型结合：集成学习的思想；
- Dropout：随机从网络中去掉一部分隐藏神经元；
- 中止方法：限制训练时间、次数，及早停止。 





### 8.2 TensorFlow.NET 代码实操 1 - DNN with Eager







### 8.3 TensorFlow.NET 代码实操 2 - DNN with Keras







### 8.4 视频教程

视频教程链接地址（或扫描下面的二维码）：





### 8.5 代码下载地址

代码下载链接地址（或扫描下面的二维码）：





